---
layout: post
title: "Artificial Neural Detector networks, experiments on MNIST"
date: 2024-05-25
---

Recently I came across an interesting scientific problem. I knew about it but never paid enough attention. The problem 
can be formulated as follows: **nature does not back propagate errors**

TO BE CONTINUED

The goal of this research is to study Artificial Neural Detector Networks in detail. I want to understand what these
models are really capable of. I plan to conduct a lot of experiments with different combinations of parameters.

This post is structured in a following way: first I describe all design choices which I plan to combine, then I provide the 
experiment results for each combination.

## ANDN design choices

### Sparse input representation

An important component of the described approach is the idea of sparse input signal representation. Each layer of the ANDN 
including the first one must deal with the sparse and binarized tensors. **Sparse** means that an input tensor 
has to contain much more zeroes than ones. **Binarized** means that all values of an input tensor must be zeroes or ones,
intermidiate numbers are not allowed. 

It is obvious that input images are not sparse and binarized. So it is not possible to use them as ANDN inputs directly and
some kind of conversion procedure is needed. According to his post Andrew Belkin solves this problem by using special untrainable layer 
which performs 16 different convolutions with 16 handcrafted kernels and groups the results into grid of 24 x 24 modules each 
containing 4x4 activations (one activation for each handcrafted kernel). Initially I tried this approach, but then I also came up with
few other ideas. 

TODO describe random kernels (explain difference between 16 fixed random kernels for all modules and 16 * 24 * 24 random kernels )
TODO describe pretrained kernels (parallel with Kohonen networks)
TODO describe pixel level sparse representation kernels

The final list of tested approaches to sparse input representation generation is:
1. Convolutions with handcrafted kernels
2. Convolutions with random kernels
3. Convolutions with pretrained kernels
4. Pixel level sparse representation

### Network topology

Due to super simple and weakly controlled learning technique a network topology becomes the most important aspect of ANDN
approach. This can also be seen as an analogy with how nature works. Millions years of evolution led to modern biological
neural architectures and the main thing that changed over time was the network topology. The basic learning technique, 
the neuron itself, was "invented" quite early and did not change much since then.
To create an ANDN network you need to answer following questions:
1. How many neuron layers should I use?
2. How big should these layers be?
3. How should I connect one layer to another?
   1. How many neuron modules do I need?
   2. How big should these modules be?
   3. How big should be receptive fields of these modules?

The variety of answers to these questions leads to an enourmous amount of combinations. I will test only those combinations
which seem conceptually different: one layer versus many layers, one small layer versus one bug layer, one module before
last layer versus many modules etc. Every design choice will be justified is some way.



## Experiments

### Unsupervised setup
#### Andrew Belkin setup



### Supervised setup
#### Andrew Belkin setup



